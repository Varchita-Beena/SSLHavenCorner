# Paper: [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003)

This paper introduces a novel system designed to generate image segmentations using various types of prompts during testing. These prompts can be either in the form of text or images. The system is trained to perform three distinct segmentation tasks, each with its own unique challenges. These tasks include referring expression segmentation, zero-shot segmentation, and one-shot segmentation.


## Outline:
1. Supervised semantic segmentation (overview)
2. Introduction
3. Model
4. PhraseCut + Visual prompts (PC+)
5. Visual Prompt Engineering
6. Metrics
7. Limitations


## Supervised semantic segmentation (overview)
The goal is to classify each pixel in an image into a specific class or category.</br>
1. Gather a dataset of images where each image is annotated with pixel level labels. These labels indicate class/category of each pixel in the image. Annotators manually label each pixel in the images with the corresponding class labels. These labels could include categories like car, tree, etc.
2. Choose a neural network architecture suitable for semantic segmentation. Common choices include U-Net, Fully Convolutional Networks (FCNs), DeepLab, and more. These architectures are designed to maintain spatial information while making predictions at different scales.
3. The most common loss function for semantic segmentation is the "Pixel-wise Cross-Entropy Loss." For each pixel, the model's predicted class probabilities are compared to the ground truth label, and the cross-entropy loss is computed. The overall loss is the average of the losses across all pixels.
4. nput: During training, the network takes an image as input.
Forward Pass: The input image goes through the network, and at the end of the architecture, the model generates a segmentation map with class scores for each pixel.
Loss Computation: The predicted segmentation map is compared to the ground truth labels using the cross-entropy loss.
Backpropagation: Gradients are computed with respect to the loss, and the weights of the network are updated using optimization algorithms like Stochastic Gradient Descent (SGD) or its variants.
5. During training, the model's performance is monitored on a validation set. The segmentation maps generated by the model are compared to the ground truth labels, and metrics like Intersection over Union (IoU) or Mean Intersection over Union (mIoU) are computed to evaluate the accuracy of the model.
6. Once the model is trained, it can be used to perform semantic segmentation on new, unseen images. The model takes an image as input and generates a segmentation map, where each pixel is assigned a class label.


## Introduction

To achieve this, the authors utilize CLIP as the foundational model. CLIP is a pre-trained model that understands visual and textual information. They enhance CLIP by integrating a transformer-based decoder, which enables the model to make dense predictions for segmentation tasks. This decoder takes the information learned by CLIP and uses it to produce detailed segmentation maps.

The model is trained using the PhraseCut dataset. This dataset contains binary segmentation maps associated with images, generated based on free-text prompts. This innovative approach allows the model to dynamically adapt not only to the specific segmentation tasks mentioned earlier but also to any binary segmentation task that can be formulated using either text or image queries. In essence, this system can generate accurate segmentations by interpreting prompts and producing segmentation maps accordingly.

The authors use the joint text-visual embedding space of CLIP to condition their model. This means they can provide prompts in the form of either text descriptions or images to guide the segmentation process.

The authors emphasize that they want the decoder to learn from CLIP's embeddings while avoiding biases from specific datasets. They aim to maintain CLIP's strong predictive capabilities while ensuring the decoder can perform accurate segmentations across various tasks.

CLIPSeg is designed for binary segmentation, where the model distinguishes between a specific foreground (which matches the prompt) and the background. This binary approach can be adapted to multi-label predictions, such as in Pascal zero-shot segmentation.

While the primary focus is on creating a versatile model, CLIPSeg is shown to achieve competitive performance across three different low-shot segmentation tasks. It can generalize to classes and descriptions for which it has never seen a segmentation before.

1. Generalized Zero-Shot Segmentation:
In this scenario, the goal is to perform image segmentation for both categories that the model has seen during training and categories that it hasn't seen before (unseen categories). To achieve this, the model uses some form of relationship between the seen and unseen categories. For instance, word embeddings or WordNet can be employed to establish connections between these categories, enabling the model to generalize its understanding and perform segmentation on unseen categories based on the knowledge it has acquired from the seen categories.

2. One-Shot Segmentation:
The model is given an additional image. This additional image represents a specific class that needs to be segmented in the query image. Along with this image, there is a mask provided. The mask indicates the exact region of interest within the additional image that corresponds to the class to be segmented. The query image is the main image that the model needs to perform segmentation on. It's the image where the desired class (that corresponds to the additional image and mask) needs to be identified and outlined. The additional image and its corresponding mask serve as a reference or guide for the model. The model analyzes the provided image and mask to understand what the target class looks like and where it's located within the image. Based on the information learned from the additional image and mask, the model then applies this knowledge to the query image. It identifies the similar features and patterns in the query image that match the target class from the additional image. The model uses this reference to perform accurate segmentation of the target class within the query image. It's like giving the model a "cheat sheet" in the form of the additional image and mask to help it accurately segment the desired class in the query image.

3. Referring Expression Segmentation:
The training process involves using detailed text queries. Each text query provides a description of a particular region or object within an image that needs to be segmented. These queries can be quite complex and descriptive, guiding the model about what to look for and segment. During the model's training phase, it is exposed to a diverse set of images and text queries, covering a wide range of categories (classes or objects). The model learns to understand the relationships between the descriptive text queries and the corresponding regions or objects within the images. When it comes to testing the model, a new image is provided along with a text query that describes a specific region or object within that image. The model uses its training experience to comprehend the query and its intended meaning. Since the model was trained on various text queries and their corresponding image regions, it knows how to interpret and follow the instructions provided in the text query. It uses this knowledge to accurately segment the area or object described in the query within the new image. Importantly, there is no requirement for the model to generalize its understanding to unseen categories. This is because during training, the model was exposed to all possible categories it might encounter. Its task is to effectively interpret the text query and perform segmentation based on what it learned from training. While referring expression segmentation is supervised, it's important to note that the supervision comes in the form of textual descriptions rather than direct pixel-wise labels. The model learns to generate segmentation masks guided by these descriptions, and its performance is evaluated based on its ability to accurately segment images based on novel text queries.

In zero-shot or few-shot segmentation scenarios, the pretrained model is not explicitly trained for all categories. Instead, it's trained on a set of categories during its initial pretraining phase. This means that the model has learned to extract features and patterns from the data, but it might not have seen examples from every possible category it might encounter later.

In zero-shot segmentation, the model is expected to generalize its understanding to segments/categories that it hasn't seen during training. For example, if the model was pretrained on images of animals and objects like cars, but it hasn't seen images of specific types of birds, it's expected to use its learned knowledge to segment those unseen categories (birds) during testing.

In few-shot segmentation, the model might have seen a few examples from a new category, but not enough to become an expert at segmenting it. It needs to quickly adapt its existing knowledge to perform segmentation on these new categories with limited examples.

Both zero-shot and few-shot segmentation are challenging tasks because they require the model to apply its learned knowledge to new or underrepresented categories. It's a way of evaluating the model's ability to generalize its understanding beyond its initial training categories.

![CLIPSeg Overview](https://github.com/Varchita-Beena/SSLHavenCorner/blob/SSLIncoming/Images/clipseg_overview.png)

## Model

###### Decoder

The decoder architecture for CLIPSeg is designed to generate image segmentations based on arbitrary prompts during test time. It employs a simple transformer-based decoder that has skip connections inspired by the U-Net architecture. These skip connections connect the decoder to the CLIP encoder, allowing the decoder to remain compact in terms of parameters.

Here's how the decoder architecture works:

1. Input Image and CLIP Visual Transformer: The query image is passed through the CLIP visual transformer, generating activations at different layers denoted as S. These activations represent different levels of abstraction and detail.

2. Extracted Activations for Decoder: Activations from certain layers S of the CLIP visual transformer are extracted and projected to the token embedding size D, which is the size of the input tokens for the decoder. These extracted activations, including the CLS token, are then added to the internal activations of the decoder before each transformer block.

3. Decoder Architecture: The decoder consists of multiple transformer blocks, where each block incorporates the extracted CLIP activations as well as its own internal activations. The decoder's last layer generates the binary segmentation by applying a linear projection on the tokens of the transformer.

4. Conditional Modulation: To inform the decoder about the segmentation target, a conditional vector is used to modulate the decoder's input activation. This modulation is achieved using Feature-wise Linear Modulation (FiLM), a technique that adapts the feature statistics of the decoder based on the conditional vector. This conditional vector can be obtained in two ways: (1) by using the CLIP text-transformer embedding of a text query or (2) by using the CLIP visual transformer on a feature-engineered prompt image.

5. Parameter Count and Image Sizes: The compactness of the decoder is emphasized, with only 1,122,305 trainable parameters for a specified token embedding size D=64. CLIP itself is not retrained but used as a feature extractor. To handle different image sizes, the positional embeddings are interpolated, allowing the model to work with varying input image dimensions.

6. Layer Selection and Activation Extraction: CLIP activations are extracted from specific layers (S = [3, 7, 9]) to be used by the decoder. In this case, the decoder has three layers that correspond to the extracted CLIP activations.

This architecture enables CLIPSeg to generate image segmentations based on various prompts, both in text and image form. The decoder's design leverages the informative features extracted by the CLIP encoder while maintaining a compact and efficient structure for segmentation tasks.

###### Image-Text Interpolation
In the CLIPSeg model, the information about what needs to be segmented (the segmentation target) is conveyed to the model through a conditional vector. This conditional vector guides the model's segmentation process by providing information about the desired outcome. This conditional vector can be obtained from either text or an image prompt.

Since CLIP uses a common embedding space for both images and text captions, it becomes possible to combine information from both modalities. This is achieved by performing an interpolation between the embeddings of an image and a text sample in the shared embedding space. This interpolation creates a new conditional vector that captures characteristics from both the image and the text.

Formally, let's denote si as the embedding of a support image and ti as the text embedding of a sample. The interpolated conditional vector xi is obtained by linearly combining the two embeddings based on a mixing coefficient a, where a is uniformly sampled from the range [0, 1]. The formula for interpolation is:

xi = a * si + (1 - a) * ti

By interpolating between the embeddings of an image and a text sample, the resulting conditional vector xi encapsulates a blend of characteristics from both sources. This interpolation method allows the model to learn from a combination of image and text information, enhancing its understanding of the segmentation task.

During training, this interpolated conditional vector serves as a form of data augmentation. By introducing randomness through the sampling of the mixing coefficient a, the model becomes more robust and adaptable to variations in input conditions. This approach leverages the shared embedding space of CLIP to bridge the gap between image and text inputs, contributing to the model's flexibility and effectiveness in segmentation tasks.

![CLIPSeg Architecture](https://github.com/Varchita-Beena/SSLHavenCorner/blob/SSLIncoming/Images/clipseg_architecture.png)

## PhraseCut + Visual prompts (PC+)
The authors utilize the "PhraseCut" dataset as the basis for their training in the CLIPSeg model. The original "PhraseCut" dataset contains phrases and corresponding object annotations. However, it does not include additional visual support information like images or image regions that could provide context for the phrases. In other words, while the dataset has descriptions (phrases) of objects, it lacks actual images or specific parts of images that correspond to those descriptions.

"Visual support samples" refer to images or image regions that are associated with the phrases in the dataset. These visual support samples help the model understand the context of the phrases by providing actual visual cues. The authors enhance the dataset by adding these visual support samples. The enhanced dataset, referred to as "PhraseCut+" or "PC+," is the result of these modifications. Unlike the original PhraseCut dataset, PC+ supports training using image-text interpolation. This means that the dataset now includes both text prompts and visual support, allowing the CLIPSeg model to be trained in a way that operates on both textual and visual inputs. The combination of these modifications extends the dataset's capabilities and suitability for the tasks outlined in the paper.

To enhance the dataset for the needs of their research, the authors make two important modifications: introducing visual support samples and negative samples.

###### Visual Support Samples:
1. Prompt ("p"): A prompt in this context refers to a text-based query or description.
2. Set "Sp":** This set contains all the samples in the dataset that share the same prompt "p."
3. Unique Prompt: If a prompt is unique, it means there's only one sample corresponding to it in the dataset.
4. Visual Support: Visual support refers to actual images or image regions that correspond to the prompt and provide context to understand it visually.
5. Multiple Samples with Same Prompt: If there are multiple samples in the dataset that share the same prompt, it means there are different instances of that prompt describing different objects.
6. Incorporating Visual Support: For prompts with multiple corresponding samples, the model selects random images from the "Sp" set. These images provide visual context that aligns with the textual prompt.

###### Negative Samples:
1. Negative Samples: These are instances where the given prompt does not correspond to any object in the image.
2. Replacing the Phrase: In these negative samples, the original phrase (description) associated with the sample is replaced by a different phrase. This is done with a certain probability "qneg."
3. Augmentation with Prefixes: The authors augment the phrases using a predefined set of prefixes. For instance, if the original phrase is "dog running," the augmented phrases could be "a dog running," "the dog running," etc.
4. Image Augmentation: For the corresponding images, random cropping is applied while considering the locations of objects. This ensures that even in negative samples, objects remain partially visible.

In summary, the authors enhance the dataset by adding visual support images to align with textual prompts, and they also introduce negative samples where phrases are replaced to create instances where prompts don't correspond to objects in the image. This augmentation provides a more comprehensive training set for the model.

## Visual Prompt Engineering
###### Masked pooling in CNN-based semantic segmentation:
In one-shot semantic segmentation using CNN a technique called 'masked pooling' is commonly used. This aims to create a prototype vector used for guiding the segmentation process. 
1. Downsampling and mask multiplication: The support mask in initially provided with the same resolution as the original input image. It's a binary mask where the ROI (the object we want to segment) is marked with 1 and the background is marked with 0.
2. In CNN, an image is passed through layers of convolution and pooling, it goes though downsampling, which reduces the spatial dimensions. These layers create feature maps that represent different levels of abstraction. One of these feature maps is chosen, usually one that retains the necessary level of detail for the segmentation task.
3. To make the support mask compatible with the chosen feature map, it needs to be resized or downsampled. This is typically done ausing interpolation techniques like bilinear interpolation. The goal is to make the mask's dimension match those of the selected feature map.
4. Once the mask is properly resized to match the feature map, an element-wise multiplication is performed between the downsized mask and the corresponding feature map. This multiplication effectively highlights the features in the feature map that correspong to the object of interest according to the support mask.

By downsizing the support mask to match the dimensions of a specific feature map, you're aligning the mask with the level of abstraction captured by that feature map. This allows the model to focus on the most relevant features when creating the prototype vector used for conditioning during the segmentation process.

###### Pooling operation:
The resulting feature map, after mask multiplication, is pooled spatially. This pooling operation aggregates the highlighted features related to the support object, creating a prototype vector that serves as a reference during segmentation. 

While masked pooling works well for CNN-based models, it can't be directly applied to transformer-based models like CLIPSeg. This is because transformers accumulate semantic information differently. In transformers, information isn't just in feature maps but also in special tokens like "CLS" tokens at different architecture levels.

In CNNs, feature maps progressively extract hierarchical features from the input image through convolution and pooling layers. Masked pooling works by highlighting these feature map regions that correspond to the support object. However, in transformers, information is accumulated in a different way. Transformers use self-attention mechanisms to consider relationships between all tokens (including special tokens like "CLS") at once, allowing them to capture complex dependencies across the entire input sequence.

Transformers use special tokens like the "CLS" token to provide context and information about the entire input sequence. These tokens are updated with information from multiple layers of the model. In contrast, CNNs don't have such tokens; instead, they rely on hierarchical features extracted from multiple convolutional layers.

Transformers' self-attention allows any token to interact with any other token, which enables the model to capture long-range dependencies and relationships. This is quite different from the localized interactions of feature maps in CNNs.

In CNN-based masked pooling, the compatibility between feature maps and the prototype vector is maintained. However, in transformer-based models like CLIP, introducing masked pooling could disrupt the compatibility between text embeddings, visual embeddings, and special tokens. Making changes to how tokens interact, like bypassing the "CLS" token, could potentially lead to a loss of overall model performance. (Interdependence between text and visual embeddings in CLIP. Making changes that break this compatibility could harm the overall performance of the model.)

## Incorporating information about the target object into the CLIP model
###### The authors want to understand how they can incorporate information about the target object into the CLIP model without directly involving segmentation tasks. They conduct an experiment to explore this.

1. They use the CLIP model to calculate text embeddings (representations) for the object names mentioned in the image. These text embeddings help describe the objects in the image.
2. Comparison Variant - 1: Original Visual Embedding (s0): This is the standard visual embedding of the original image.
3. Comparison Variant - 2: Highlighted Visual Embedding (sh): This represents the visual embedding of the image with the target object highlighted. The highlighting can be achieved by modifying the RGB image to draw attention to the target object or by using an attention mask to emphasize the target object.

"Highlighted Visual Embedding" (sh) is achieved through modifying the RGB image and using an attention mask:
A. Modifying RGB Image:
a. In this approach, the original image is taken, and modifications are made to draw attention to the target object.</br>
b. One common technique is to change the color of the target object to make it stand out from the rest of the image. This can be done by altering the color channels of the pixels corresponding to the target object while keeping the other pixels unchanged.</br>
c. Another technique is to add visual effects like borders, shadows, or other graphical elements around the target object to make it more prominent.</br>
d. The goal is to visually highlight the target object so that it's easily distinguishable from the rest of the scene.</br>

Using an Attention Mask:
a. An attention mask is a binary mask that indicates which regions of the image are most important or should be emphasized.</br>
b. To create an attention mask for the target object, the area corresponding to the target object is set to 1 (indicating high attention), while the rest of the image is set to 0 (low attention).</br>
c. When this attention mask is applied to the image, the target object is visually highlighted, as it stands out due to the higher attention assigned to it.</br>
d. The attention mask directs the model's focus to the target object when processing the image, which can lead to better alignment between the text description of the object and its visual representation.</br>
In the context of using an attention mask to highlight the region of interest, there is no downsampling involved. The process involves multiplying the original image with the attention mask to emphasize the specific region of interest while keeping the rest of the image unchanged.</br>

4. The authors measure the "alignment" between the text embeddings and the visual embeddings. Alignment is calculated using the cosine distance, a metric that indicates how similar two embeddings are.
5. The alignment values are normalized using a softmax function, which transforms them into probability distributions. This helps in comparing the impact of different highlighting techniques.
6. To evaluate the effectiveness of highlighting, the authors focus on the text embedding of the target object. They expect that this text embedding should align more strongly with the highlighted image embedding than with the original image embedding (s0). In other words, if the highlighting technique is effective, there should be a noticeable increase in the probability of the target object being correctly identified.
7. The experiment uses the LVIS dataset, which contains images with multiple objects and a variety of categories.
8. The authors select 1,600 images from the dataset. For each image, they mask out one target object. This means they temporarily remove one object from the image and analyze how well the model can align embeddings without directly segmenting the object.

In summary, this experiment focuses on exploring the alignment between visual and text embeddings in the CLIP model when different techniques are used to highlight the target object. By analyzing how these highlighting techniques impact alignment, the authors gain insights into how target information can be integrated into CLIP without the complexity of segmentation tasks.

###### CLIP-Based Masking: 
1. In a visual transformer, there is a set of tokens that represent various parts of an image. These tokens interact with each other through multi-head attention, allowing them to exchange information at different layers of the transformer.
2. The authors wanted to incorporate masking, similar to the concept of masked pooling in CNN-based models, into the visual transformer. The idea is to apply a mask on these tokens in a way that controls their interactions during the multi-head attention process.
3. The mask integration involves constraining the interactions between tokens based on the mask. This can be achieved by limiting the interactions in one or more layers of the transformer to specific tokens. These tokens include the CLS token (used for read-out) and the image-region-related tokens (originally obtained from image patches).
4. To evaluate the effectiveness of this approach, they conducted experiments and measured performance. The evaluation suggests that the straightforward approach of introducing the mask by constraining interactions with specific tokens did not yield significant improvements.
5. The authors analyzed the impact of constraining interactions with different tokens. When interactions were constrained only with the CLS token, a small improvement was observed, but it was not substantial. On the other hand, constraining interactions with all tokens dramatically decreased performance.
6. Based on these results, the authors concluded that more complex strategies are needed to effectively combine the image and the mask internally within the transformer. This means that simply applying a mask to specific tokens in the visual transformer is not sufficient to achieve the desired segmentation results. More sophisticated methods are required to ensure that the interactions between tokens and the mask are well-coordinated and lead to improved performance.

Mask integration involves modifying the interactions between tokens based on the mask. Tokens that correspond to the highlighted regions in the mask are given special attention during these interactions. During each attention mechanism, the model computes attention scores that determine how much each token should focus on other tokens. When integrating a mask, the attention scores for tokens inside the highlighted region might be increased, making them more influential during computations. As information passes through the layers of the model, tokens associated with the highlighted regions receive more attention from other tokens. This allows the model to capture the important features within those regions. the mask acts as a guiding mechanism for the transformer's attention. By highlighting the region of interest in the mask, we guide the model to focus more on that region during its computations. 

###### Visual Prompt Engineering
The mask and the original image are combined to create a new image, which is then fed into the visual transformer for processing. This technique is analogous to the concept of prompt engineering used in natural language processing, such as in models like GPT-3.

In natural language processing (NLP), prompt engineering involves designing specific text prompts to guide the behavior of language models. In this context, the authors draw an analogy and apply a similar concept to images. They manipulate the input image and mask to create a new image that serves as a more effective prompt for the visual transformer.

The authors experimented with various ways of designing these visual prompts. They evaluated different techniques for combining the mask and the image. They tested different operations on the image, such as decreasing the background brightness, blurring the background using a Gaussian filter, and cropping the image to focus on the object.

They found that a combination of three operations decreasing background brightness, blurring the background, and cropping to the object yielded the best results in terms of alignment improvement. Apply the three transformations over the image, then over mask on this modified image gives us new image. 

## Metrics
In the context of zero-shot and one-shot segmentation, an "open vocabulary" means that the model is not constrained to a fixed set of predefined classes or expressions. This is in contrast to approaches where the model can only predict among a predetermined set of categories. In the case of this study, the model can generate predictions for a wide range of classes or expressions, and this set is not predetermined.

The models in this study are trained to generate binary predictions. This binary prediction can later be transformed into a multi-label setting if needed, where multiple labels can be assigned to a single image.

1. IoU - It measures the overlap between the predicted segmented region and the actual segmented region. It's calculated as the area of overlap divided by the area of union between the predicted and ground truth regions.

2.Foreground IoU (IoUFG) -  This variant of IoU calculates the intersection over union considering only the pixels that belong to the foreground region (object of interest). It focuses on how well the model segments the object itself.

3. Mean IoU: This metric computes the average IoU over different classes' foreground regions. It gives an overall idea of how well the model is performing across various classes.

4. Binary IoU (IoUBIN) - In binary segmentation, where the task is to predict the presence or absence of the object, IoUBIN averages IoU over both foreground and background regions.

5. Threshold for IoU - IoU requires a threshold to determine whether a prediction is considered a match or not. A common threshold is 0.5, where predictions with an IoU above 0.5 are considered correct. However, the choice of this threshold can be crucial and can vary depending on the task and model. The optimal threshold can strongly deviate from 0.5 if there's a difference in the object probability distribution between training and inference.

6. Average Precision (AP) - AP is a metric that evaluates how well a model can distinguish between correct and incorrect predictions across a range of thresholds. It's measured by calculating the area under the recall-precision curve. It provides an understanding of the model's ability to rank predictions effectively and is independent of the specific threshold chosen.

## Limitations
The authors acknowledge that their experiments have been conducted on a small number of benchmark datasets. This implies that the results and conclusions drawn from these experiments might not necessarily generalize to all possible scenarios. 

The model's pre-training relies on a large-scale dataset called CLIP. While this dataset is a valuable resource, the authors note that using it for pre-training introduces a potential limitation. The model's performance could be influenced by the characteristics and diversity of the data in the CLIP dataset. Additionally, the authors mention that they don't utilize the best-performing CLIP model (ViT-L/14@336px) due to the availability of model weights.

The model's current scope is primarily centered around images. While it excels in this domain, the model's applicability to video data might face challenges related to temporal consistency. Videos involve sequences of frames, and ensuring that the model captures meaningful temporal relationships could be more complex than in the image domain.

Although the model accommodates different image sizes, there are practical limits to how much the image size can vary.









